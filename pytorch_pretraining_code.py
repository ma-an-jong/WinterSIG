#!/usr/bin/env python
# coding: utf-8

# In[ ]:


#í—ˆê¹…í˜ì´ìŠ¤ íŠ¸ëœìŠ¤í¬ë¨¸ ì„¤ì¹˜
get_ipython().system('pip install -q git+https://github.com/huggingface/transformers.git')


# In[ ]:


import io
import os
import math
import torch
import warnings
from tqdm.notebook import tqdm
from transformers import (
                          CONFIG_MAPPING,
                          MODEL_FOR_MASKED_LM_MAPPING,
                          MODEL_FOR_CAUSAL_LM_MAPPING,
                          PreTrainedTokenizer,
                          TrainingArguments,
                          AutoConfig,
                          AutoTokenizer,
                          AutoModelWithLMHead,
                          AutoModelForCausalLM,
                          AutoModelForMaskedLM,
                          LineByLineTextDataset,
                          TextDataset,
                          DataCollatorForLanguageModeling,
                          DataCollatorForWholeWordMask,
                          DataCollatorForPermutationLanguageModeling,
                          PretrainedConfig,
                          Trainer,
                          set_seed,
                          )
 
set_seed(123)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')


# In[ ]:


device


# In[ ]:


#ì¡´ì¬í•˜ëŠ” dataset ê°€ì ¸ì˜¤ê¸°
with open('train.txt','r',encoding = 'utf-8') as f:
    data = f.readlines()


# In[ ]:


data[0:5]


# In[ ]:


class ModelDataArguments(object):
  r"""
    ëª¨ë¸ì„ ì •ì˜í•˜ê³  ì‚¬ì „í•™ìŠµì— ì‚¬ìš©í•  ë°ì´í„° config ì‘ì„±
    
   ëª¨ë“  argsëŠ” optional, í•˜ì§€ë§Œ íŠ¹ì • ìˆ«ìë‚˜ ê°’ì„ ìš”êµ¬í•˜ëŠ” ê²½ìš° ì¡´ì¬
   ex) bert-base layerê°œìˆ˜ , ì€ë‹‰ì¸µ ê°œìˆ˜ , max_seq_len í†µì¼ ë“±
   
  args:
    train_data_file (:obj:`str`, `optional`): í›ˆë ¨í•  ë°ì´í„°ì˜ ìœ„ì¹˜
    íŒŒì¼ì´ lineë‹¨ìœ„ë¡œ ì´ë£¨ì–´ ì ¸ìˆë‹¤ë©´ line_by_line=Trueë¡œ ì„¤ì •
    ë§Œì•½ ë°ì´í„° íŒŒì¼ì´ íŠ¹ë³„í•œ ë¬¶ìŒìœ¼ë¡œ ì´ë£¨ì–´ì ¸ìˆì§€ ì•Šìœ¼ë©´ Falseë¡œ ì„¤ì •
    defaultë¡œ Noneìœ¼ë¡œ ì„¤ì •
 
    eval_data_file (:obj:`str`, `optional`): í‰ê°€ ë°ì´í„°ì˜ ìœ„ì¹˜
    train_data_fileê³¼ ë™ì¼
    defaultë¡œ Noneìœ¼ë¡œ ì„¤ì •
 
    line_by_line (:obj:`bool`, `optional`, defaults to :obj:`False`): 
      í•™ìŠµ ë°ì´í„°ì…‹ê³¼ ê²€ì¦ ë°ì´í„°ì…‹ì´ ë¼ì¸ë‹¨ìœ„ë¡œ êµ¬ë¶„ëœë‹¤ë©´ 
      êµ¬ë¶„ì´ ì—†ë‹¤ë©´ Falseë¡œ ì§€ì •
 
    mlm (:obj:`bool`, `optional`, defaults to :obj:`False`): 
    ëª¨ë¸ êµ¬ì¡°ì— ì˜ì¡´ì ì¸ ì†ì‹¤í•¨ìˆ˜ë¥¼ ë³€ê²½í• ì§€ ì •í•˜ëŠ” flag
    ì´ ë³€ìˆ˜ëŠ” masked language modelì˜ ê²½ìš° Trueë¡œ ì„¤ì •í•´ì•¼ í•˜ë©° ë‹¤ë¥¸ ê²½ìš°ëŠ” Falseë¡œ ì§€ì •
    ì´ë ‡ê²Œ ì„¤ì •í•˜ì§€ ì•Šìœ¼ë©´ ValueErrorë¥¼ ì¼ìœ¼í‚¤ëŠ” í•¨ìˆ˜ê°€ ì„¤ì •ë˜ì–´ìˆìŒ
 
    whole_word_mask (:obj:`bool`, `optional`, defaults to :obj:`False`):
    ì „ì²´ ë‹¨ì–´ ë§ˆìŠ¤í‚¹ ì‚¬ìš© ì—¬ë¶€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” flag
    ì „ì²´ ë‹¨ì–´ ë§ˆìŠ¤í‚¹ì´ë€ í•™ìŠµì¤‘ ëª¨ë“  ë‹¨ì–´ê°€ maskingì´ ë ìˆ˜ìˆëŠ”ì§€
 
    mlm_probability(:obj:`float`, `optional`, defaults to :obj:`0.15`): 
    masked language model ì´ í•™ìŠµì¤‘ì— ì‚¬ìš©
    mlm=Trueì¼ë•Œ í•„ìš”ë¡œí•¨
    ë§ˆìŠ¤í‚¹ë˜ëŠ” tokenì˜ ë¹„ìœ¨ì„ í‘œê¸°
 
    plm_probability (:obj:`float`, `optional`, defaults to :obj:`float(1/6)`): 
      permutation language modelingì„ ìœ„í•œ contextë¥¼ ê°ì‹¸ê³  ìˆëŠ” ë§ˆìŠ¤í‚¹ëœ í† í°ì˜ í­ì˜ ê¸¸ì´ì˜ ë¹„ìœ¨ì„ ì •ì˜
      XLNetì„ ì‚¬ìš©í• ë•Œ í•„ìš”
 
    max_span_length (:obj:`int`, `optional`, defaults to :obj:`5`): 
    permutation language modelingì—ì„œ ìµœëŒ€ ë§ˆìŠ¤í‚¹ëœ í† í°ì˜ ê¸¸ì´ ì œí•œì„ ì„¤ì •
    XLNetì„ ì´ìš©í• ë•Œ í•„ìš”.
 
    block_size (:obj:`int`, `optional`, defaults to :obj:`-1`): 
      í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ì´ë™í•˜ëŠ” ìœˆë„ìš° ì‚¬ì´ì¦ˆë¥¼ ë‚˜íƒ€ëƒ„
      -1 ~ max_seq_len ê¹Œì§€ ì‚¬ìš©ì´ ê°€ëŠ¥

    overwrite_cache (:obj:`bool`, `optional`, defaults to :obj:`False`): 
      ë‹¤ë¥¸ ìºì‹œíŒŒì¼ì´ ì¡´ì¬í•œë‹¤ë©´ ë®ì–´ì“°ê¸°
 
    model_type (:obj:`str`, `optional`): 
      Type of model used: bert, roberta, gpt2. 
      More details: https://huggingface.co/transformers/pretrained_models.html
      defaultë¡œ None ì„¤ì •
 
    model_config_name (:obj:`str`, `optional`):
      Config of model used: bert, roberta, gpt2. 
      More details: https://huggingface.co/transformers/pretrained_models.html
      defaultë¡œ None ì„¤ì •
 
    tokenizer_name: (:obj:`str`, `optional`)
      ë°˜ë“œì‹œ í•™ìŠµì‹œí‚¬ ëª¨ë¸ê³¼ ê°™ì€ ì´ë¦„ìœ¼ë¡œ tokenizerí˜¸ì¶œ í•´ì•¼ë¨ 
      It usually has same name as model_name_or_path: bert-base-cased, 
      roberta-base, gpt2 etc.
      defaultë¡œ None ì„¤ì •.
 
    model_name_or_path (:obj:`str`, `optional`): 
      Path to existing transformers model or name of 
      transformer model to be used: bert-base-cased, roberta-base, gpt2 etc. 
      More details: https://huggingface.co/transformers/pretrained_models.html
      defaultë¡œ None ì„¤ì •
 
    model_cache_dir (:obj:`str`, `optional`): 
      ì¬ ì‹¤í–‰ í• ë•Œ ë¹ ë¥¸ ì „ê°œë¥¼ ìœ„í•œ .cache ë””ë ‰í† ë¦¬ ê²½ë¡œ
 
  Raises:
        ValueError: If `CONFIG_MAPPING` is not loaded in global variables.
        ValueError: If `model_type` is not present in `CONFIG_MAPPING.keys()`.
        ValueError: If `model_type`, `model_config_name` and 
          `model_name_or_path` variables are all `None`. At least one of them 
          needs to be set.
          
        warnings: If `model_config_name` and `model_name_or_path` are both 
          `None`, the model will be trained from scratch.
          
        ValueError: If `tokenizer_name` and `model_name_or_path` are both 
          `None`. We need at least one of them set to load tokenizer.
  """
 
  def __init__(self, 
               train_data_file=None,
               eval_data_file=None, 
               line_by_line=False,
               mlm=False,
               mlm_probability=0.15, 
               whole_word_mask=False,
               plm_probability=float(1/6), 
               max_span_length=5,
               block_size=-1,
               overwrite_cache=False, 
               model_type=None,
               model_config_name=None,
               tokenizer_name=None, 
               model_name_or_path=None,
               model_cache_dir=None):
     
    # CONFIG_MAPPINGì´ transformerë¡œ ë¶€í„°  importê°€ ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if 'CONFIG_MAPPING' not in globals():
      raise ValueError('Could not find `CONFIG_MAPPING` imported! Make sure'                        ' to import it from `transformers` module!')
 
    # model_typeì´ ìœ íš¨í•œì§€ í™•ì¸
    if (model_type is not None) and (model_type not in CONFIG_MAPPING.keys()):
      raise ValueError('Invalid `model_type`! Use one of the following: %s' %
                       (str(list(CONFIG_MAPPING.keys()))))
       
    # model_type, model_config_name and model_name_or_path ë“±ì´ Noneìœ¼ë¡œ ì„¤ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸
    if not any([model_type, model_config_name, model_name_or_path]):
      raise ValueError('You can`t have all `model_type`, `model_config_name`,'                        ' `model_name_or_path` be `None`! You need to have'                        'at least one of them set!')
    
    # scratchë¡œ ë¶€í„° ìƒˆë¡œìš´ ëª¨ë¸ì´ Load ë˜ëŠ”ì§€ í™•ì¸
    if not any([model_config_name, model_name_or_path]):
      # ê²½ê³ ì°½ ì„¤ì •
      warnings.formatwarning = lambda message,category,*args,**kwargs:                                '%s: %s\n' % (category.__name__, message)
      #  ê²½ê³  ì¶œë ¥
      warnings.warn('You are planning to train a model from scratch! ğŸ™€')
 
    # ìƒˆë¡œìš´ Tokenizerê°€ ìƒì„±ë˜ëŠ”ì§€ í™•ì¸ , but ì œëŒ€ë¡œ ì§€ì›í•˜ì§€ ì•ŠìŒ
    if not any([tokenizer_name, model_name_or_path]):
      #scratch í•œ tokenizerëŠ” í•™ìŠµì‹œí‚¬ìˆ˜ ì—†ì–´ì„œ ì˜¤ë¥˜ë¥¼ ë°œìƒì‹œí‚´
      raise ValueError('You want to train tokenizer from scratch! '                     'That is not possible yet! You can train your own '                     'tokenizer separately and use path here to load it!')
       
    # agrs ì„¤ì •
    self.train_data_file = train_data_file
    self.eval_data_file = eval_data_file
    self.line_by_line = line_by_line
    self.mlm = mlm
    self.whole_word_mask = whole_word_mask
    self.mlm_probability = mlm_probability
    self.plm_probability = plm_probability
    self.max_span_length = max_span_length
    self.block_size = block_size
    self.overwrite_cache = overwrite_cache
 
    # model,tokenizer ë§¤ê°œë³€ìˆ˜ ì„¤ì •
    self.model_type = model_type
    self.model_config_name = model_config_name
    self.tokenizer_name = tokenizer_name
    self.model_name_or_path = model_name_or_path
    self.model_cache_dir = model_cache_dir
     
    return


# In[ ]:


# data ì„¤ì • arguments
model_data_args = ModelDataArguments(
                                    train_data_file='train.txt', 
                                    eval_data_file='test.txt', 
                                    line_by_line=False, 
                                    mlm=True,
                                    whole_word_mask=True,
                                    mlm_probability=0.15,
                                    plm_probability=float(1/6), 
                                    max_span_length=5,
                                    block_size=50,
                                    overwrite_cache=False,
                                    model_type='bert',
                                    model_config_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', 
                                    tokenizer_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', 
                                    model_name_or_path='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', 
                                    model_cache_dir=None,
                                    )


# In[ ]:


def get_model_config(args: ModelDataArguments):
  r"""

  í•™ìŠµí•  ëª¨ë¸ì˜ configë¥¼ ê°€ì ¸ì˜´

  ModelDataArgumentë¥¼ ì‚¬ìš©í•˜ì—¬ model configë¥¼ ìš”ì²­í•˜ì—¬ ê°€ì ¸ì˜´
  
  ë§¤ê°œë³€ìˆ˜:
    args (:obj:`ModelDataArguments`):
      ëª¨ë¸,ë°ì´í„° config ë§¤ê°œë³€ìˆ˜ë“¤ì€ ì‚¬ì „ í•™ìŠµì— í•„ìš”
  Returns:
    :obj:`PretrainedConfig`: Model_transformers_configuration. ex.bert_config
     
  Raises:
    ValueError: If `mlm=True` and `model_type` is NOT in ["bert", 
          "roberta", "distilbert", "camembert"]. We need to use a masked 
          language model in order to set `mlm=True`.
          
          masked language model = Trueì¸ë° ëª¨ë¸ì´ bert,distil,camem ë“± ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ì˜ íŒŒìƒì´ ì•„ë‹ˆë©´ ì•ˆë¨
  """
 
  # model config ì²´í¬
  if args.model_config_name is not None:
        
    # 1.cofigë¡œ ë¶€í„° ì´ë¦„ì„ ì–»ì–´ì™€ì„œ ì‚¬ìš©
    model_config = AutoConfig.from_pretrained(args.model_config_name, 
                                      cache_dir=args.model_cache_dir)
 
  elif args.model_name_or_path is not None:
    # 2.configë¡œ ë¶€í„° ê²½ë¡œë¥¼ ì–»ì–´ì™€ì„œ ì‚¬ìš©
    model_config = AutoConfig.from_pretrained(args.model_name_or_path, 
                                      cache_dir=args.model_cache_dir)
 
  else:
    # ë§Œì•½ scratchí•œ ëª¨ë¸ì´ë¼ë©´ config mappingì„ ì‚¬ìš©
    model_config = CONFIG_MAPPING[args.model_type]()
 
  # mlmì˜µì…˜ì´ MASKED LANGUAGE MODELì„ ìœ„í•œ ì„¤ì •ì´ ë˜ì–´ìˆëŠ”ì§€ í™•ì¸
  if (model_config.model_type in ["bert", "roberta", "distilbert", 
                                  "camembert"]) and (args.mlm is False):
    raise ValueError('BERT and RoBERTa-like models do not have LM heads '                     'butmasked LM heads. They must be run setting `mlm=True`')
   
  # xlnetì´ë¼ë©´ ë§ëŠ” block_sizeë¥¼ ì„¤ì •
  if model_config.model_type == "xlnet":
    # xlnetì€ ì‚¬ì „ í•™ìŠµì— block_size = 512ë¡œ ì„¤ì •
    args.block_size = 512
    # setup memory length
    model_config.mem_len = 1024
   
  return model_config


# In[ ]:


# model configuration.
print('Loading model configuration...')
config = get_model_config(model_data_args)


# In[ ]:


config


# In[ ]:


def get_tokenizer(args: ModelDataArguments):
  r"""
  Tokenizer ì–»ì–´ì˜¤ëŠ” ë©”ì†Œë“œ

  ModelArgumentsë¥¼ ì´ìš©í•˜ì—¬ ëª¨ë¸ì˜ tokenizerë¥¼ ìƒì„± ë° block_sizeë“±, argsë¥¼ ë³€ê²½ í•  ìˆ˜ ìˆìŒ

  ë§¤ê°œë³€ìˆ˜:
    args (:obj:`ModelDataArguments`):
      Model and data configuration arguments needed to perform pretraining.
      ëª¨ë¸ê³¼ data config argumentsëŠ” ì‚¬ì „í•™ìŠµì— í•„ìš”ë¡œ í•¨
      
  Returns:
    :obj:`PreTrainedTokenizer`: Model transformers tokenizer.
  """

  # tokenizer config ì²´í¬
  if args.tokenizer_name:
    # 1.tokenizerì˜ ì´ë¦„ìœ¼ë¡œ ìƒì„±
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, 
                                              cache_dir=args.model_cache_dir)
 
  elif args.model_name_or_path:
    # 2.tokenizerì˜ ê²½ë¡œë¡œ ìƒì„±
    tokenizer = AutoTokenizer.from_pretrained(args.model_name_or_path, 
                                              cache_dir=args.model_cache_dir)
     
  # dataì˜ block_sizeë¥¼ ì„¤ì •
  if args.block_size == 0:
    # tokenizerì˜ ìµœëŒ€ ê¸¸ì´ë¡œ block_sizeë¥¼ ì„¤ì •
    # ì…ë ¥ block_sizeëŠ” modelì´ ì…ë ¥ì˜ ìµœëŒ€ì¹˜ê¹Œì§€ ê°€ëŠ¥
    # ê°€ë” max_lengths ê°€ ë§¤ìš° í¬ë©´ ë¬¸ì œê°€ ë°œìƒê°€ëŠ¥
    args.block_size = tokenizer.model_max_length
  else:
    # tokenizerì˜ max_lengthë¥¼ ë„˜ì–´ì„œì§€ ë§ê²ƒ
    args.block_size = min(args.block_size, tokenizer.model_max_length)
 
  return tokenizer


# In[ ]:


tokenizer = get_tokenizer(model_data_args)


# In[ ]:


tokenizer


# In[ ]:


def get_model(args: ModelDataArguments, model_config):
  r"""
  ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
  ModelDataArgumentsë¥¼ ì´ìš©í•˜ì—¬ ì‹¤ì œ ëª¨ë¸ì„ ë¦¬í„´
  
  ë§¤ê°œë³€ìˆ˜:
    args (:obj:`ModelDataArguments`):
      ëª¨ë¸ê³¼ data config ë§¤ê°œë³€ìˆ˜ëŠ” ì‚¬ì „í•™ìŠµì— í•„ìš”
      
    model_config (:obj:`PretrainedConfig`):
      Model transformers configuration.
      
  Returns:
    :obj:`torch.nn.Module`: â˜…PyTorchâ˜… model.
 
  """
 

  # MODEL_FOR_MASKED_LM_MAPPING ê³¼ MODEL_FOR_CAUSAL_LM_MAPPING ì´ transformers ë¡œë¶€í„° import ë˜ì–´ì•¼í•¨
  if ('MODEL_FOR_MASKED_LM_MAPPING' not in globals()) and                 ('MODEL_FOR_CAUSAL_LM_MAPPING' not in globals()):
    raise ValueError('Could not find `MODEL_FOR_MASKED_LM_MAPPING` and'                      ' `MODEL_FOR_MASKED_LM_MAPPING` imported! Make sure to'                      ' import them from `transformers` module!')
     

  # ì‚¬ì „ í•™ìŠµ ëª¨ë¸ ì‚¬ìš© ë˜ëŠ” scratchë¡œ ë¶€í„° í›ˆë ¨ì¸ì§€ í™•ì¸
  if args.model_name_or_path:
    # ì‚¬ì „ í•™ìŠµ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°
    if type(model_config) in MODEL_FOR_MASKED_LM_MAPPING.keys():
      # Masked language modeling head.
      return AutoModelForMaskedLM.from_pretrained(
                        args.model_name_or_path,
                        from_tf=bool(".ckpt" in args.model_name_or_path),
                        config=model_config,
                        cache_dir=args.model_cache_dir,
                        )
    elif type(model_config) in MODEL_FOR_CAUSAL_LM_MAPPING.keys():
      # Causal language modeling head.
      return AutoModelForCausalLM.from_pretrained(
                                          args.model_name_or_path, 
                                          from_tf=bool(".ckpt" in
                                                        args.model_name_or_path),
                                          config=model_config, 
                                          cache_dir=args.model_cache_dir)
    else:
      raise ValueError(
          'Invalid `model_name_or_path`! It should be in %s or %s!' %
          (str(MODEL_FOR_MASKED_LM_MAPPING.keys()), 
           str(MODEL_FOR_CAUSAL_LM_MAPPING.keys())))
     
  else:
    # Use model from configuration - train from scratch.
      print("Training new model from scratch!")
      return AutoModelWithLMHead.from_config(config)


# In[ ]:


print('Loading actual model...')
model = get_model(model_data_args, config)


# In[ ]:


def get_dataset(args: ModelDataArguments, tokenizer: PreTrainedTokenizer, 
                evaluate: bool=False):
  r"""
  ë°ì´í„°ì…‹ì„ pytorch Datasetìœ¼ë¡œ ë§Œë“¤ì–´ì¤Œ
 
  Using the ModelDataArguments return the actual model.
  ModelDataArgumentsì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ modelì„ return
  
  args: 
    args (:obj:`ModelDataArguments`):

    tokenizer (:obj:`PreTrainedTokenizer`):
      
    evaluate (:obj:`bool`, `optional`, defaults to :obj:`False`):
      If set to `True` the test / validation file is being handled.
      True = í…ŒìŠ¤íŠ¸/ê²€ì¦ ì„¸íŠ¸ False = í›ˆë ¨ ì„¸íŠ¸
  Returns:
    íŒŒì¼ ë°ì´í„°ë¥¼ ê°€ì§„ Pytorch Dataset
  """

  # train ë˜ëŠ” evaluate íŒŒì¼ ê²½ë¡œë¥¼ ì–»ì–´ì˜´
  file_path = args.eval_data_file if evaluate else args.train_data_file
 
  # line_by_lineì´ Trueì¸ì§€ í™•ì¸
  if args.line_by_line:
    #ë°ì´í„° íŒŒì¼ì— ì¡´ì¬í•˜ëŠ” ê° exampleì€ ê° ë¼ì¸ì„
    return LineByLineTextDataset(tokenizer=tokenizer, file_path=file_path, 
                                 block_size=args.block_size)
     
  else:
    #íŒŒì¼ë‚´ ëª¨ë“  ë°ì´í„°ëŠ” ë¶„ë¦¬ ì—†ì´ í•¨ê»˜ ì¡´ì¬
    return TextDataset(tokenizer=tokenizer, file_path=file_path, 
                       block_size=args.block_size, 
                       overwrite_cache=args.overwrite_cache)


# In[ ]:


# í•™ìŠµì— í•„ìš”í•œ argumentsë¥¼ ì„¤ì •
"""
Note: 

I only used the arguments I care about.
`TrainingArguments` contains a lot more arguments.

For more details check the awesome documentation:

https://huggingface.co/transformers/main_classes/trainer.html#trainingarguments
"""

training_args = TrainingArguments(
                       #ì²´í¬í¬ì¸íŠ¸ì™€ ëª¨ë¸ predictionì´ ì“°ì—¬ì§ˆ ë””ë ‰í† ë¦¬ ê²½ë¡œ ì„¤ì •
                         output_dir='pretrain_kobert',
                       #ì¶œë ¥ ë””ë ‰í† ë¦¬ì˜ contentë¥¼ ë®ì–´ì“°ê¸° í• ì§€ ì„¤ì •
                         overwrite_output_dir=True,
   
                       #trainning,evaluateì„ í• ê²ƒì¸ê°€
                         do_train=True, 
                         do_eval=True, 
   
                       # Batch size GPU/TPU core/CPU training,evaluation.
                         per_device_train_batch_size=16,
                         per_device_eval_batch_size=100,
   
                         # trainingë„ì¤‘ evaluation ì ìš© ë°©ë²•
                             # `no`: í•™ìŠµë„ì¤‘ í‰ê°€ë¥¼ í•˜ì§€ ì•ŠëŠ”ë‹¤.
                             # `steps`: `eval_steps` ë§ˆë‹¤ í‰ê°€ë¥¼ ì§„í–‰.
                             # `epoch`: ë§¤ epochì´ ëë‚ ë•Œë§ˆë‹¤ í‰ê°€.
                         evaluation_strategy='steps',

                       #ì–¼ë§ˆë‚˜ ìì£¼ logë¥¼ ë„ìš¸ê²ƒì¸ê°€, lossì™€ perplexityì˜ ê¸°ë¡
                         logging_steps=2000,
   
                       #ë§Œì•½ evaluationg_strategy = "steps"ì´ë©´ evalê³¼ evalê°„ì˜ update stepsì˜ ê°œìˆ˜
                       #ì„¤ì •ì„ í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ logging_stepê³¼ ê°™ì€ ê°’ìœ¼ë¡œ ì„¤ì •
                         eval_steps = None,
                          
                       #Trueë¡œ ì„¤ì •ì‹œ perplexity ê³„ì‚°ì„ ìœ„í•œ ì†ì‹¤ì„ ì¶œë ¥
                         prediction_loss_only=True,

                       #Adamì„ ìœ„í•œ learning_rateë¥¼ ì„¤ì • ê¸°ë³¸ê°’ 5e-5
                         learning_rate = 5e-5,

                       #Weight decayëŠ” í•™ìŠµëœ ëª¨ë¸ì˜ ë³µì¡ë„ë¥¼ ì¤„ì´ê¸° ìœ„í•´ì„œ
                       #í•™ìŠµ ì¤‘ weightê°€ ë„ˆë¬´ í° ê°’ì„ ê°€ì§€ì§€ ì•Šë„ë¡ 
                       #Loss functionì— Weightê°€ ì»¤ì§ˆê²½ìš°ì— ëŒ€í•œ íŒ¨ë„í‹°ë¥¼ ì ìš©
                         weight_decay=0,
   
                       #epsilonì€ Adam optimizerì—ì„œ ë¶„ëª¨ê°€ 0ì´ ë˜ì§€ ì•Šë„ë¡ í•˜ê¸° ìœ„í•œ ì‘ì€ ê°’ 
                       # ê¸°ë³¸ê°’ì€ 1e-8
                         adam_epsilon = 1e-8,

                       # ê·¸ë˜ë””ì–¸íŠ¸ clippingì„ ìœ„í•œ ìµœëŒ€ ê·¸ë˜ë””ì–¸íŠ¸ norm
                       # ê¸°ë³¸ê°’ì€ 0
                         max_grad_norm = 1.0,
   
                       # epoch
                         num_train_epochs = 3,

                       # ì²´í¬í¬ì¸íŠ¸ ê°„ê²©  
                       # ê¸°ë³¸ê°’ 500
                         save_steps = 2000,
                         )


# In[ ]:


# train dataset ì„¤ì •
print('Creating train dataset...')
train_dataset = get_dataset(model_data_args, tokenizer=tokenizer, evaluate=False) if training_args.do_train else None
 
# eval dataset ì„¤ì •
print('Creating evaluate dataset...')
eval_dataset = get_dataset(model_data_args, tokenizer=tokenizer, evaluate=True) if training_args.do_eval else None


# In[ ]:


train_dataset[0:10]


# In[ ]:


def get_collator(args: ModelDataArguments, model_config: PretrainedConfig, 
                 tokenizer: PreTrainedTokenizer):
  r"""
  ì í•©í•œ collator í•¨ìˆ˜ë¥¼ ê°€ì ¸ì˜´
  Collator í•¨ìˆ˜ëŠ” Pytorch Dataset ê°ì²´ë¥¼ ìˆ˜ì§‘í•˜ëŠ”ë° ì´ìš©ë¨
  
  ë§¤ê°œë³€ìˆ˜:
    args (:obj:`ModelDataArguments`):
      Model ê³¼ data config ë§¤ê°œë³€ìˆ˜ëŠ” ì‚¬ì „í•™ìŠµì— í•„ìš”í•¨
      
    model_config (:obj:`PretrainedConfig`):
      Model transformers configuration.
      
    tokenizer (:obj:`PreTrainedTokenizer`):
      Model transformers tokenizer.

  Returns:
    :obj:`data_collator`: Transformers specific data collator.
 
  """
 
  # Special dataset handle depending on model type.
  # íŠ¹ë³„í•œ datasetì€ ëª¨ë¸ typeì— ì˜ì¡´í•˜ì—¬ ê´€ë¦¬í•¨
    
  if model_config.model_type == "xlnet":
    # XLNETë¥¼ ìœ„í•œ collator
    return DataCollatorForPermutationLanguageModeling(
                                          tokenizer=tokenizer,
                                          plm_probability=args.plm_probability,
                                          max_span_length=args.max_span_length,
                                          )
  else:
    # ë‚˜ë¨¸ì§€ modelì„ ìœ„í•œ data
    if args.mlm and args.whole_word_mask:
      # ì „ì²´ ë‹¨ì–´ ë§ˆìŠ¤í‚¹ì„ ì‚¬ìš©
      return DataCollatorForWholeWordMask(
                                          tokenizer=tokenizer, 
                                          mlm_probability=args.mlm_probability,
                                          )
    else:
      # ì¼ë°˜ì ì¸ ì–¸ì–´ ëª¨ë¸
      return DataCollatorForLanguageModeling(
                                          tokenizer=tokenizer, 
                                          mlm=args.mlm, 
                                          mlm_probability=args.mlm_probability,
                                          )


# In[ ]:


data_collator = get_collator(model_data_args, config, tokenizer)


# In[ ]:


# Trainer ìƒì„±
print('Loading `trainer`...')
trainer = Trainer(model=model,
                  args=training_args,
                  data_collator=data_collator,
                  train_dataset=train_dataset,
                  eval_dataset=eval_dataset,
                  )
 
 

 # ì €ì¥ì„ ìœ„í•œ ëª¨ë¸ ê²½ë¡œ
if training_args.do_train:
  print('Start training...')
 
  # í›ˆë ¨í•  ëª¨ë¸ì´local pathë¥¼ í†µí•´ ë¡œë”©ë˜ì—ˆë‹¤ë©´ modelê²½ë¡œë¥¼ ì„¤ì •
  model_path = (model_data_args.model_name_or_path 
                if model_data_args.model_name_or_path is not None and
                os.path.isdir(model_data_args.model_name_or_path) 
                else None
                )
  # í•™ìŠµ ì‹œì‘
  trainer.train(model_path=model_path)
  # ëª¨ë¸ ì €ì¥
  trainer.save_model()
 
  # í¸ì˜ë¥¼ ìœ„í•´ì„œ ìš°ë¦¬ëŠ” tokenizerë¥¼ ê°™ì€ ê²½ë¡œì— ë‹¤ì‹œ ì €ì¥í•¨
  # ê·¸ë˜ì„œ í•™ìŠµí•œ ëª¨ë¸ì„ ì‰½ê²Œ ê³µìœ ê°€ ê°€ëŠ¥
  if trainer.is_world_process_zero():
    tokenizer.save_pretrained(training_args.output_dir)


# In[ ]:


# do_evalì´ Trueì¸ì§€ í™•ì¸
if training_args.do_eval:
    
# trainerì˜ í‰ê°€ ê²°ê³¼
  eval_output = trainer.evaluate()

#ëª¨ë¸ì˜ lossë¥¼ í†µí•´ perplexityë¥¼ ê³„ì‚°
  perplexity = math.exp(eval_output["eval_loss"])
  print('\nEvaluate Perplexity: {:10,.2f}'.format(perplexity))

else:
  print('No evaluation needed. No evaluation data provided, `do_eval=False`!')


# In[ ]:


trainer.save_model()


# In[ ]:


if trainer.is_world_process_zero():
    tokenizer.save_pretrained(training_args.output_dir)


# In[ ]:




